%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% IMPLEMENTATION %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this chapter, we present in details our implementation of the model transformer that we exposed in the previous chapters. As programming language and runtime platform we use Java. As modeling and code generation tool we use Eclipse Modeling Framework\footnote{www.eclipse.org/modeling/emf} (EMF).

A general view of the model transformer procedure is depicted in Figure \ref{fig:implementation-scheme}. The inputs of the procedure consist of a source model in the EMF format, to be transformed into a target model, and a PAC BNCE TGG that describes the transformation between source and target models. These inputs are processed by two sub-procedures, \emph{EMF to Graph} and \emph{NP Normalization}, that generate a graph and a NP PAC BNCE TGG, respectively, if possible. These sub results are, then, forwarded to the \emph{Parsing} step, that parses (see Algorithms \ref{alg:parse} and \ref{alg:parsepac}) the input graph and produces a derivation, which is, then, consumed by the \emph{Production} step to create the output triple graph beholding both source and target models (see Algorithm \ref{alg:transform}). These four sub-procedures are elucidated in the next paragraphs.

\begin{figure}[h]
	\input{misc/implementation-scheme}
	\caption{Implementation scheme for the model transformer presented by this thesis. The inputs, \emph{Source Model} and \emph{PAC BNCE TGG}, are depicted in the left as rectangles. The four sub-procedures \emph{EMF to Graph}, \emph{NP Normalization}, \emph{Parsing} and \emph{Production} are depicted in the middle as rounded-corner restangles. The output \emph{Triple Graph} is depicted in the right.}
	\label{fig:implementation-scheme}
\end{figure}

\paragraph*{\emph{EMF to Graph}.} The \emph{EMF to Graph} step transforms the source model, that is in the EMF format, into a graph. This can be done trivially, for it is a one-to-one transformation, where each element from the model is transformed into a vertex of the graph. It suffices, thus, to trespass the source model, element by element, starting from the roots up to the elements whose all children have already been trespassed. At each visited element, a vertex and an edge to each of its children is created.

\paragraph*{\emph{NP Normalization}.} The \emph{NP Normalization} step normalizes the input PAC BNCE TGG, so that it fits the neighborhood preserving (NP) normal form. This normalization consists of creating a triple graph grammar $TGG'$ equivalent to the input grammar $TGG$, i.e. $L(TGG') = L(TGG)$, for which the NP property (see Definition \ref{def:np}) holds. An NP Normalization algorithm for NLC graph grammars can be found in \citep{rozenberg1986boundary}. We adapt this algorithm for NCE graph grammars and use it to normalize the source part of our input TGG (see Definition \ref{def:source}).

The NP Normalization starts by looking for non-NP rules. For each of these rules, it modifies its left-hand side so that it becomes NP. Moreover, it also adds new rules, that are produced by replacing each occurrence of the old left-hand side by the new one. This procedure is then repeated until the grammar is not modified anymore. It is guaranteed that it always stops producing a NP PAC BNCE graph grammar. After stopping, the NP Normalization also modifies the input TGG, by adding and modifying the correspondent triple rules whose source parts were modified in the process.

\paragraph*{\emph{Parsing}.} The result of the steps \emph{EMF to Graph} and \emph{NP Normalization}, that are the source graph to be transformed and a normalized TGG, are used by the step \emph{Parsing} to produce a valid derivation for the source graph, in case it can be transformed following the rules in the TGG. Sections \ref{sec:parsing} and \ref{sec:pac-parsing} already offer abstract presentations of the parsing algorithms for BNCE and PAC BNCE graph grammars. Thus, in the following paragraphs, we explore more concrete issues that come along with the implementation of the parser.

The parsing procedure can be seen as a search algorithm that explores systematically the search space of all parsing trees for the TGG until it finds the parsing tree for the input graph. It is easy to see that such search space is huge (and potentially infinite) for any practical TGG. The parser starts from the trivial parsing trees, each one containing only one zone vertex of the source graph (see Line 3 in Algorithm \ref{alg:parsepac}). Then, at every time that it finds a new derivation (see Line 9), it augments its search space with a new parsing tree for the just found derivation (see Line 11). Additionally, the parser also holds the so-called $bup$ set with the zone vertices found by derivations assembled from other zone vertices in $bup$, which also grows at each time a derivation is found (see Line 10).

Notice, thus, that the direction to where the search space grows depends on the choice of which subset of zone vertices are picked from $bup$ (see Line 5) as a handle to find new derivations. Notice also that the number of possibilities for such a choice explodes as the size of $bup$ grows, hence, the runtime tends to grow too, as $bup$ grows. Indeed, the function that describes this growth, despite its importance in the complexity analysis of the parsing algorithm, is not exactly known, as far as we know. Although, \cite[p. 160]{rozenberg1986boundary} points out that this function is a polynomial on the size of the source graph, for connected degree-bounded graphs.

Anyway, in an attempt to cope with this problem, we implemented three different strategies to query the $bup$ set: The \emph{naive}, the \emph{greedy} and the \emph{greedy aware} strategies. In the \emph{naive} strategy, the selection of subsets from $bup$ are performed from the smaller to the bigger ones, without any other criterion. In the \emph{greedy} strategy, subsets containing zone vertices added after the initialization get higher priority. Moreover, from those subsets, the ones with the greater amount of vertices (i.e. the greatest) get an even higher priority and are served first. Finally, the \emph{greedy aware} strategy extends the \emph{greedy} strategy, by using information about the graph being parsed and the grammar being utilized.

In this strategy, beyond the size criterion of the zone vertices, subsets that contain more zone vertices closer to the biggest zone vertex in it are ranked better. More specifically, the selector takes the biggest zone vertex and sums the approximate distance from the other zone vertices to it. The lesser this sum is, the better the subset is ranked to be queried. The approximate distance between two zone vertices $z$ and $y$ is $k$, if the least undirected path from any vertex of $z$ to any vertex of $y$ is $k$ and $k \leq 2$; otherwise it is 4.

Beyond the distance criterion, the \emph{greedy aware} strategy also uses the depth information of each vertex to decide on the priority of subsets that tied in the previous criteria. That is, subsets with zone vertices that contain deeper vertices are prioritized. The depth of a vertex $v$, in this case, is the length of the first path that got to $v$ in the \emph{EMF to Graph} transformation. Finally, for a source grammar that has no rules with a $k$-sized right-hand side, this strategy never chooses subsets with exact $k$ zone vertices.

In general, the use of the \emph{greedy} strategy entails the growth of the search space in the direction of greater parsing trees because of the size criterion, what potentially makes it get to the final parsing tree in fewer steps. The \emph{greedy aware} strategy explores locality through the assumption that a derivation is more likely to be found with a handle containing nearer zone vertices, reducing the amount of effort with subsets that do not generate new zone vertices. Such assumption is supported by the fact that rules' right-hand sides tend to be connected in practical TGG. Lastly, it prioritizes deeper zone vertices following the observation that grammars are often built in such a way that deeper vertices (i.e. vertices that are further from the root vertex) tend to occur deeper in parsing tress too. In such case, specially for the beginning of the parsing, a deeper zone vertex may entail the generation of parsing trees that are more likely to be the correct ones and end up reducing the search effort considerably.

Our belief is that the \emph{greedy aware} strategy outperforms the other two alternatives in average, although we also suppose that some strategy may suit better the parsing of some classes of graphs or grammars. The former expectation is supported only by some brief experimental evaluations, what does not entitles us to affirm that firmly.

More strategies, besides the three ones presented here, could be created, including, for example, the implementation of meta-heuristics, like simulated annealing, to guide the parsing tree search in a more robust fashion.

Regarding the parallelization of the parsing procedure, it is possible to parallelize it with as many threads as wished. We do it by having a central manager for the $bup$ set, that retrieves subsets and adds zone vertices to the $bup$ upon requests and according to the strategy. This central manager receives such requests from the concurrent threads, that effectively evaluate a subset with zone vertices in search of new derivations.

In this parallel architecture, enhancements can be done to decrease the synchronization time required at the central $bup$ manager upon addition of new zone vertices. This operation is specially costly because the addition of a new zone vertex implies the creation of new subsets and the insertion of them in an ordered queue. Such addition has a worst-case time complexity greater than constant in our implementation, asymptotically.

In general, our parser implementation could become more efficient also through the reduction of heavy memory operations executed by the copy of zone vertices throughout the parsing process, that are not essentially necessary. Furthermore, experimental profile analysis indicate that the isomorphism checks consume a considerable amount of time. This isomorphism checks are necessary to verify that a handle can be generated by a derivation step (Line 7 and 9 of Algorithm \ref{alg:parsepac}) and, although its time complexity is a function on the maximal size of the handle--- i.e. the maximal size of the right-hand sides of the grammar's rules--- which tends to be much smaller than the size of the source graph, the overhead produced by it is still considerable. 

\paragraph*{\emph{Production}.} To finalize the whole transformation procedure, the step \emph{Production} takes the derivation of the source graph found by the parser to create a triple graph whose source part is identical to the source graph (up to isomorphism) and whose target part holds the desired transformed graph, as exposed in Chapters \ref{ch:ModelTransformation} and \ref{ch:pac-extension}. This can be done, practically, by a two-pass method. First, a triple graph with unresolved PAC vertices is created step-by-step by iterating in the derivation, applying at each derivation step the respective triple rule on the respective vertices and creating at each derivation step a resolution step that maps the just created PAC vertices to their respective vertices in the source graph. At the second pass, these resolution steps are iterated in such a way that, at each step, a resolution step is applied to solve the respective PAC vertices.

Notice that, here our implementation deviates slightly from the theory, in which our parser produces a derivation and not a production. That is the reason why the step \emph{Production} is responsible for building the resolution steps. This comes in handy, because the construction of each resolution step requires the construction of the resolution mapping that takes from a PAC vertex to a real vertex. But this real vertex is only determined after its effective creation, what is executed only by the \emph{Production} step. Hence, after creating such vertices, it can construct the resolution function much easier.

