%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% IMPLEMENTATION %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%TODO: Change refs from algorithm 1 to the algorithm with PAC
In this chapter, we present in details our implementation for the model transformer that we exposed in the previous chapters. As programming language and runtime platform we use Java. As modeling and code generation tool we use Eclipse Modeling Framework (EMF).

%TODO: Maybe explain the TGG and TG metamodels (with image maybe)

The model transformer procedure is depicted in Figure \ref{fig:implementation-scheme}. The input for the procedure consists of a source model, which is an instance of the source metamodel, and a TGG model, which is an instance of the TGG metamodel. The source model represents the model to be transformed into a target model and the TGG model holds the BNCE triple graph grammar that describes the transformation between the source metamodel and a target metamodel. The output model is an instance of the TG metamodel and holds the triple graph generated by the transformation procedure.

\begin{figure}[h]
	\input{misc/implementation-scheme}
	\caption{Implementation scheme for the model transformer presented by this thesis}
	\label{fig:implementation-scheme}
\end{figure}

\paragraph*{\emph{Ecore to Graph}.} The source model is transformed into a graph by the step \emph{Ecore to Graph}. This can be done trivially, for it is a one-to-one transformation, where each element from the model is transformed into a vertex of the graph. It suffices, thus, to trespass the source model element by element, starting from the roots up to the elements whose all children have already been trespassed. At each visited element, a vertex and an edge to each of its children is created.

\paragraph*{\emph{NP Normalization}.} The TGG model is normalized to fit the neighborhood preserving (NP) normal form by the step \emph{NP Normalization}. This normalization consists of creating an triple graph grammar $TGG'$ equivalent to $TGG$, i.e. $L(TGG') = L(TGG)$, for which the NP property (see Definition \ref{def:np}) holds. An NP Normalization algorithm for NLC graph grammars can be found in \citep{rozenberg1986boundary}. We adapt this algorithm for NCE graph grammars and use it to normalize the source part of the TGG model (see Definition \ref{def:source}).

The NP Normalization starts by looking for non-NP rules. For each of these rules, it modifies its left-hand side so that it becomes NP. Moreover, it also adds new rules, that are produced by replacing each occurrence of the old left-hand side by the new one. This procedure is then repeated until the grammar is not modified anymore. It is guaranteed that it always stops producing a NP NCE graph grammar. After stopping, the NP Normalization also modifies the TGG model adding and modifying the correspondent triple rules whose source parts were modified in the process.

%TODO: Enhancement: Normalizer to make grammar smaller

\paragraph*{\emph{Parsing}.} The result of the steps \emph{Ecore to Graph} and \emph{NP Normalization}, that are the source graph to be transformed and a normalized TGG, are used by the step \emph{Parsing} to produce a valid derivation for the source graph, in case it can be transformed following the rules in the TGG. Section \ref{sec:parsing} already offers an abstract presentation of the parsing algorithm for BNCE graph grammar. Thus, in the following paragraphs, we explore more concrete issues that come along with the implementation of the parser.

The parsing procedure can be seen as a search algorithm that explores systematically the search space of all parsing trees for the TGG until it finds the parsing tree for the input graph. It is easy to see that such search space is huge (and potentially infinite) for any practical TGG. The parser starts from the trivial parsing trees, each one containing only one zone vertex of the source graph (see Line 3 in Algorithm \ref{alg:parsepac}). Then, at every time that it finds a new derivation (see Line 9), it augments its search space with a new parsing tree for the just found derivation (see Line 11). Additionally, the parser also holds the so-called $bup$ set with the zone vertices found by derivations assembled from other zone vertices in $bup$, which also grows at each time a derivation is found (see Line 10).

Notice, thus, that the direction to where the search space grows depends on the choice of which subset of zone vertices are picked from $bup$ (see Line 5) as a handle to find new derivations and that the number of possibilities for such choice explodes as the size of $bup$ grows. Indeed, the function that describes this growth, despite its importance in the complexity analysis of the parsing algorithm, is not known, as far as we know, but it is a polynomial on the size of the source graph \cite[p. 160]{rozenberg1986boundary}.

%TODO: By now it should be clear that bups are chose to match with RHSs

Therefore, we implemented three different strategies to query the $bup$ set: The \emph{naive}, the \emph{greedy} and the \emph{greedy aware}. In the \emph{naive} strategy, the selection of subsets from $bup$ are performed from the smaller to the bigger ones, without any other criterion. In the \emph{greedy} strategy, subsets containing zone vertices added after the initialization get higher priority. Moreover, from those subsets, the ones with the greater amount of vertices (i.e. the greatest) get an even higher priority and are served first. Finally, the \emph{greedy aware} strategy extends the \emph{greedy} strategy, by using information about the graph being parsed and the grammar being utilized.

In this strategy, beyond the size criterion of the zone vertices, subsets that contain more zone vertices closer to the biggest zone vertex in it are ranked better. More specifically, the selector takes the biggest zone vertex and sums the approximate distance from the other zone vertices to it. The lesser this sum is, the better the subset is ranked to be queried. The approximate distance between two zone vertices $z$ and $y$ is $k$, if the least undirected path from any vertex of $z$ to any vertex of $y$ is $k$ and $k \leq 2$; otherwise it is 4.

Beyond the distance criterion, the \emph{greedy aware} strategy also uses the depth information of each vertex to decide on the priority of subsets that tied in the previous criteria. That is, subsets with zone vertices that contain deeper vertices are prioritized. The depth of a vertex $v$, in this case, is the length of the first path that got to $v$ in the \emph{Ecore to Graph} transformation. And, finally, this strategy does not had over subsets with $k$ zone vertices, where the source grammar has no rule with a $k-$sized right-hand side.

In general, the use of a \emph{greedy} strategy entails the growth of the search space in the direction of greater parsing trees because of the size criterion, what potentially makes it get to the final parsing tree in fewer steps. A \emph{greedy aware} strategy, moreover, explores locality through the assumption that a derivation is more likely to be found with a handle containing nearer zone vertices, reducing the amount of effort with subsets that do not generate new zone vertices. Such assumption is supported by the fact that rules' right-hand sides tend to be connected in practical TGG. Lastly, it prioritizes deeper zone vertices following the observation that often grammars are built in such a way that deeper vertices (i.e. vertices that are further from the root vertex) tend to occur deeper in parsing tress too. In such case, specially for the beginning of the parsing, a deeper zone vertex may entail the generation of parsing trees that are more likely to be the correct ones and end up reducing the search effort considerably.

Our belief is that the \emph{greedy aware} strategy outperforms the other two alternatives in average, although we also suppose that some strategy may suit better the parsing of some classes of graphs or grammars. The former expectation is supported by one of our brief experimental evaluations but we cannot affirm that firmly, for a more detailed study would be necessary for that end.

More strategies besides the three ones presented here could be created, including, for example, the implementation of meta-heuristics, like the simulated annealing, to guide the parsing tree search in a more robust manner.

Regarding the parallelization of the parsing procedure, it is possible to parallelize it with as many threads as wished. We do it by having a central manager for the $bup$ set, that retrieves subsets and adds zone vertices to the $bup$ upon requests and according to the strategy. This central manager receives such requests from the concurrent threads, that effectively evaluate a subset with zone vertices in search of new derivations.

In this parallel architecture, enhancements can be done to decrease the synchronization time required by the central $bup$ manager at the addition of new zone vertices. This operation is specially costly because the addition of a new zone vertex implies the creation of new subsets and the insertion of them in an ordered queue. Such addition has a worst-case time complexity greater than constant in our implementation.

In general our parser implementation could become more efficient also through the reduction of heavy memory operations executed by the copy of zone vertices throughout the parsing process, that are not essentially necessary. Furthermore, experimental profile analysis indicate that the isomorphism checks consume a considerable amount of time. This isomorphism checks are necessary to verify that a handle can be generated by a derivation step (Line 7 and 9 of Algorithm \ref{alg:parsepac}) and, although its time complexity is a function on the maximal size of the handle--- i.e. the maximal size of the right-hand sides of the grammar's rules--- which tends to be much smaller than the size of the source graph, the overhead produced by it is still considerable. 

\paragraph*{\emph{Production}.} To finalize the whole transformation procedure, the step \emph{Production} takes the derivation of the source graph found by the parser to create a triple graph whose source part is identical to the source graph (up to isomorphism) and the target part holds the desired transformed graph, as exposed in Chapter \ref{ch:ModelTransformation}. This can be done, practically, by a two-pass method. First, a triple graph with unresolved PAC vertices is created step-by-step by iterating in the derivation, applying at each derivation step the respective triple rule on the respective vertices and creating at each derivation step a resolution step that maps the just created PAC vertices to their respective vertices in the source graph. At the second pass, these resolution steps are iterated in such a way that, at each step, a resolution step is applied to solve the respective PAC vertices.

%TODO: Maybe be cleare about the morphisms here in the resolution steps. i.e. be more formal
%TODO: Maybe add remark, to say implementation differs from theory, in that parser produces a derivation and not a production.
