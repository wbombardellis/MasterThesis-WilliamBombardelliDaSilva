%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%% IMPLEMENTATION %%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this chapter, we present in details our implementation for the model transformer that we exposed in the previous chapters. As programming language and runtime platform we use Java. As modeling and code generation tool we use Eclipse Modeling Framework (EMF).

%TODO: Maybe explain the TGG and TG metamodels (with image maybe)

The model transformer procedure is depicted in Figure \ref{fig:implementation-scheme}. The input for the procedure consists of a source model, which is an instance of the source metamodel, and a TGG model, which is an instance of the TGG metamodel. The source model represents the model to be transformed into a target model and the TGG model holds the BNCE triple graph grammar that describes the transformation between the source metamodel and a target metamodel.

\begin{figure}[h]
	\input{misc/implementation-scheme}
	%TODO
	\caption{...}
	\label{fig:implementation-scheme}
\end{figure}

The source model is transformed into a graph by the step \textit{Ecore to Graph}. This can be done trivially, for it is a one-to-one transformation, where each element from the model is transformed into a vertex of the graph. It suffices, thus, to trespass the source model element by element, starting from the roots up to the elements whose all children have already been trespassed. At each visited element, a vertex and an edge to each of its children is created.

The TGG model is normalized to fit the neighborhood preserving (NP) normal form by the step \textit{NP Normalizer}. This normalization consists of creating an triple graph grammar $TGG'$ equivalent to $TGG$, i.e. $L(TGG') = L(TGG)$, for which the NP property (see Definition \ref{def:np}) holds. An NP normalizer algorithm for NLC graph grammars can be found in \citep{rozenberg1986boundary}. We adapt this algorithm for NCE graph grammars and use it to normalize the source part of the TGG model (see \ref{def:source}).

The NP normalizer starts by looking for non-NP rules. For each of these rules, it modifies its left-hand side so that it becomes NP. Moreover, it also adds new rules, that are produced by replacing each occurrence of the old left-hand side by the new one. This procedure is then repeated until the grammar is not modified anymore. It is guaranteed that it always stops producing a NP NCE graph grammar. After stopping, the NP normalizer also modifies the TGG model adding and modifying the correspondent triple rules whose source parts were modified in the process.

%TODO: Enhancement: Normalizer to make grammar smaller

The result of the steps \textit{Ecore to Graph} and \textit{NP Normalizer}, that are the source graph to be transformed and a normalized TGG, are used by the step \textit{Parsing} to produce a valid derivation for the source graph, in case it can be transformed following the rules in the TGG. Section \ref{sec:parsing} already offers an abstract presentation of the parsing algorithm for BNCE graph grammar. Thus, in the following paragraphs, we explore more concrete issues that come along with the implementation of the parser.

The parsing procedure can be seen as a search algorithm that explores systematically the search space of all parsing trees for the TGG until it finds the parsing tree for the input graph. It is easy to see that such search space is huge (and potentially infinite) for any practical TGG. The parser starts from the trivial parsing trees, each one containing only one zone vertex of the source graph (see Line 3 in Algorithm \ref{alg:parse}). Then, at every time that it finds a new derivation (see Line 9), it augments its search space with a new parsing tree for the just found derivation (see Line 11). Additionally, the parser also holds the so-called $bup$ set with the zone vertices found by derivations assembled from other zone vertices in $bup$, which also grows at each time a derivation is found (see Line 10).

Notice, thus, that the direction to where the search space grows depends on the choice of which subset of zone vertices are picked from $bup$ (see Line 5) as a handle to find new derivations and that the number of possibilities for such choice explodes as the size of $bup$ grows. Indeed, the function that describes this growth, despite its importance in the complexity analysis of the parsing algorithm, is not known, as far as we know, but it is a polynomium on the size of the source graph \cite[p. 160]{rozenberg1986boundary}.

%TODO: By now it should be clear that bups are chose to match with RHSs

Therefore, we implemented three different strategies to query the $bup$ set: The \textit{naive}, the \textit{greedy} and the \textit{greedy aware}. In the \textit{naive} strategy, the selection of subsets from $bup$ are performed from the smaller to the bigger ones, without any other criterion. In the \textit{greedy} strategy, subsets containing zone vertices added after the initialization get higher priority. Moreover, from those subsets, the ones with the greater amount of vertices (i.e. the greatest) get an even higher priority and are served first. Finally, the \textit{greedy aware} strategy extends the \textit{greedy} strategy, by using information about the graph being parsed and the grammar being utilized.

In this strategy, beyond the size criterion of the zone vertices, subsets that contain more zone vertices closer to the biggest zone vertex in it are ranked better. More specifically, the selector takes the biggest zone vertex and sums the approximate distance from the other zone vertices to it. The lesser this sum is, the better the subset is ranked to be queried. The approximate distance between two zone vertices $z$ and $y$ is $k$, if the least undirected path from any vertex of $z$ to any vertex of $y$ is $k$ and $k \leq 2$; otherwise it is 4.

Beyond the distance criterion, the \textit{greedy aware} strategy also uses the depth information of each vertex to decide on the priority of subsets that tied in the previous criteria. That is, subsets with zone vertices that contain deeper vertices are prioritized. The depth of a vertex $v$, in this case, is the length of the first path that got to $v$ in the \textit{Ecore to Graph} transformation. And, finally, this strategy does not had over subsets with $k$ zone vertices, where the source grammar has no rule with a $k-$sized right-hand side.
%TODO: Explain why use the depth and taboo k

In general, the use of a \textit{greedy} strategy entails the growth of the search space in the direction of greater parsing trees because of the size criterion, what potentially makes it get to the final parsing tree in fewer steps. A \textit{greedy aware} strategy, moreover, explores locality through the assumption that a derivation is more likely to be found with a handle containing nearer zone vertices, reducing the amount of effort with subsets that do not generate new zone vertices. Such assumption is supported by the fact that rules' right-hand sides tend to be connected in practical TGG. Lastly, it prioritizes deeper zone vertices following the observation that often grammars are built in such a way that deeper vertices (i.e. vertices that are further from the root vertex) tend to occur deeper in parsing tress too. In such case, specially for the beginning of the parsing, a deeper zone vertex may entail the generation of parsing trees that are more likely to be the correct ones and end up reducing the search effort considerably.

Our belief is that the \textit{greedy aware} strategy outperforms the other two alternatives in average, although we also suppose that some strategy may suit better the parsing of some classes of graphs or grammars. The former expectation is supported by one of our brief experimental evaluations but we cannot affirm that firmly, for a more detailed study would be necessary for that end.

More strategies besides the three ones presented here could be created, including, for example, the implementation of meta-heuristics, like the simulated annealing, to guide the parsing tree search in a more robust manner.


%TODO: Enhancement: too many memory operations

